---
title: 'Circular Features Across Domains: Exploring Gemma Models’ Internal Representations'
date: '2025-01-27'
tags: ['interpretability']
draft: false
summary: In this project I explored how LLMs like Google's Gemma models internally represent circular concepts, from weekdays and months to colors and musical notes, building on recent discoveries about geometric patterns in these models' internal structures.
---

## Summary
This project was completed as part of the AI Safety Fundamentals course. I attempted to replicate some of the experimental results from the paper [Not All Language Model Features Are Linear](https://arxiv.org/abs/2405.14860) (Engels et al., 2024), but by using different models than the original authors. 

The paper demonstrated that certain features were represented circularly within the model's internal structure. One of their experiments involved prompting a language model with lists of weekdays and months, using temporal modifiers like "early in" or "late in," which revealed a continuous circular pattern in the representations.

Using Google's open-weight Gemma-2 models (2B, 9B and 27B), I partially managed to replicate these results. More specifically, I showed that the circular patterns for weekdays occur in all three (?) models, while it was not as clear for the months setup.

The other aim of the project was to extend this experiment with other potentially circular concepts - colour spaces and musical scales - and examining the structure of their activations. Here I found that encoding colours in the HSV (Hue, Saturation, Value) colour space indeed did show circular tendencies for the mid-sized Gemma model, while the other novel experiments didn’t show any clear patterns.

This project was modest in scope, but I hope this can serve as a tiny incremental contribution to the AI interpretability field, which is trying to reduce potential harmful outcomes by increasing our understanding of inner workings of (in this case) large language models.

The experiment code can be found in [this repository](https://github.com/sgenas/circular-features-experiments/tree/master).

## Introduction
Why would we be interested in what happens within the the myriad pathways inside a language model? There has probably been curiosity among model developers and enthusiasts, but I would imagine that for the vast majority of people, there is no interest in cracking open the black box as long as the chatbot’s answers are useful.

But the AI safety community has a different perspective. Interpretability research - sometimes prefixed as *mechanistic* interpretability to highlight the microscopic and causal interests of the field - has been suggested as a potential way to alleviate some risks that capable AI systems pose. A topic [summary from the AI Safety Fundamentals course](https://aisafetyfundamentals.com/blog/introduction-to-mechanistic-interpretability/) puts it succinctly:

> Significant progress in interpreting neural networks would afford us insight into the rationale behind decisions that impact large numbers of people. […] Progress in mechanistic interpretability is also one path to detecting unaligned behaviour in AI models. If we can only assess a model’s outputs, but not the processes that give rise to them, we may not be able to tell whether a model is genuinely acting in the best interests of its user.
> 

So, interpretability is about understanding, on a mathematical level, how the internal representations of a model is affecting its outputs, and to be able to “read its thoughts” by reading the internal, intermediate numbers - understanding architecture in action.

Recent work by Engels et al. in [this paper](https://arxiv.org/abs/2405.14860) showed that language models seem to encode certain concepts in a circular way, and they showed that the models also *use* these representations in modular arithmetic operations. This paper investigates whether language models use multi-dimensional (rather than just one-dimensional) representations to encode information and perform computations.

The key contributions of the paper were:

1. They developed mathematical definitions for identifying "true" multi-dimensional features (as opposed to features that could be broken down into independent one-dimensional components).
2. They created methods to automatically find such features using so-called sparse autoencoders (SAEs).
3. The authors discovered that language models represent certain concepts using circular, two-dimensional features - particularly for days of the week and months of the year. These aren't just static representations - the models actively use these circular features to solve calendar-based arithmetic problems.

I found it fascinating to see these results and decided that this paper would be the starting point of this project. And in short, the guiding research questions have been:

- Is it possible to find circular representations of weekdays and months in the Gemma models as well?
- Is it possible to find any other concepts that the models encode in a circular way?

## Experiments and Results
I developed a framework to run the experiment from the paper, but which also permitted other inputs than the original ones. The core process was to:

1. Generate embeddings for various prompts involving circular concepts
2. Extract hidden states from different layers of the models
3. Perform dimensionality reduction using principal component analysis (PCA) on the activations
4. Visualize in two dimensions using by plotting the top two PCA components

This was in line with the last experiment of Engels et al. - I basically just rewrote and extended their code ([here](https://github.com/JoshEngels/MultiDimensionalFeatures/blob/main/sae_multid_feature_discovery/other_circle_points.py) is a link to their script, and [here](https://github.com/sgenas/circular-features-experiments/blob/master/src/continuity_experiments.py) is a link to mine).

I ran the experiments for four different concept domains:

- Weekdays
- Months
- Colour spaces
- Musical notes

And I did this with three different models (all non-quantized base models from HuggingFace):

- [Gemma-2 2B](https://huggingface.co/google/gemma-2-2b)
- [Gemma-2 9B](https://huggingface.co/google/gemma-2-9b)
- [Gemma-2 27B](https://huggingface.co/google/gemma-2-27b)

### Key takeaways
1. I successfully replicated the circular patterns for weekdays across all three Gemma models (2B, 9B, 27B), particularly in early layers. However, the month/season circular patterns were less clear compared to the original Engels et al. paper
- The modifier ordering ("early"/"late") was less consistent than in the original study.

### Weekday Experiments
I generated prompts ranging from "very early on Monday" to "very late on Sunday". The key findings were:

- Similar circular representations appeared in the Gemma models, particularly in early layers
- Weekdays maintained their sequential order in the representations
- The ordering of modifiers ("early"/"late") was less consistent than in the original paper

export const fileNameWeekday = 'weekday_very';

<PCAVisualization dataPath={`/data/circular-features/gemma-2-2b_${fileNameWeekday}_merged.json`}/>

Here's another PCA visualization showing the relationships between different items, but with Gemma 2 9B instead:

<PCAVisualization dataPath={`/data/circular-features/gemma-2-9b_${fileNameWeekday}_merged.json`}/>

And here's the one with Gemma 2 27B:

<PCAVisualization dataPath={`/data/circular-features/gemma-2-27b_${fileNameWeekday}_merged.json`}/>

### Month/Season Experiments
Using prompts with modifiers "early in" and "late in" for months and seasons:

- Some layers showed structured patterns
- The clear circular arrangements from the original paper were not consistently replicated
- This suggests the representation might be model-architecture dependent

export const fileNameMonth = 'month';

<PCAVisualization dataPath={`/data/circular-features/gemma-2-2b_${fileNameMonth}_merged.json`}/>

Here's another PCA visualization showing the relationships between different items, but with Gemma 2 9B instead:

<PCAVisualization dataPath={`/data/circular-features/gemma-2-9b_${fileNameMonth}_merged.json`}/>

And here's the one with Gemma 2 27B:

<PCAVisualization dataPath={`/data/circular-features/gemma-2-27b_${fileNameMonth}_merged.json`}/>

### Color Space Experiments
For the colour space experiments, I started with a simple set of strings with base colors to see if the model would represent them in a way that reflects their natural circular relationship. Here are the test prompts for this experiments:

```python
base_colors = [
    "Violet",
    "Blue",
    "Cyan",
    "Green",
    "Yellow",
    "Orange",
    "Red"
]
```

This experiment revealed no clear circular patterns in the data. In the few cases where points did form circular arrangements, they failed to align with the expected ordering found in the [RGB color wheel](https://en.m.wikipedia.org/wiki/File:RGB_color_wheel.svg).

So I decided to try out and see if colours represented in the [HSV](https://en.wikipedia.org/wiki/HSL_and_HSV) (*Hue, Saturation, Value*) colour space would “work”, since this system actually explicitly represents the colour hues in terms of degrees between 0 and 360.

The test prompts for this setup was:
```python
hsv_colors = [
    "HSV 0 100 100 colour values",    # Red
    "HSV 60 100 100 colour values",   # Yellow
    "HSV 120 100 100 colour values",  # Green
    "HSV 180 100 100 colour values",  # Cyan
    "HSV 240 100 100 colour values",  # Blue
    "HSV 300 100 100 colour values",  # Magenta
]
```


export const fileNameHSV = 'hsv_colour_one_red';

<PCAVisualization dataPath={`/data/circular-features/gemma-2-2b_${fileNameHSV}_merged.json`}/>

Here's another PCA visualization showing the relationships between different items, but with Gemma 2 9B instead:

<PCAVisualization dataPath={`/data/circular-features/gemma-2-9b_${fileNameHSV}_merged.json`}/>

And here's the one with Gemma 2 27B:

<PCAVisualization dataPath={`/data/circular-features/gemma-2-27b_${fileNameHSV}_merged.json`}/>

The colours would form a circle (or polygon at least) that had the colours in order for most of the layers in the 9B model, especially in the latter part of the layers - for a cherry-picked example, check out layer 38. But there were no clear circular patterns with the colors in order for the other two models.

I also tried adding tertiary colors in the experiment, which is the above colour values but with the intermediary colors as well (`Yellow-Green / Chartreuse`, `Green-Cyan / Spring Green`, `Cyan-Blue / Azure`, `Violet` and `Red-Magenta / Rose`, with hue values 30, 90, 150 and so on). But this configuration showed no clear patterns at all.

### Musical Note Experiments
I explored two variations:

1. Full chromatic scale (C, C#, D, D#, ..., A#, B)
2. C-major scale (C, D, E, F, G, A, B) with "flat"/"sharp" modifiers

- No clear circular patterns emerged
- Sharp notes (#) tended to cluster together, possibly due to tokenization effects

- Almost no circular patterns detected
- Note ordering was not preserved in the geometric arrangement

## Closing thoughts
In the paper, there’s this (wonderfully technical) line about another study that found multidimensional features:

> However, our work finds circular features that represent latent concepts from text, while the GPT-2 learned position vectors [from that other study] are specific to tokenization, separate from the rest of the model parameters, and causally implicated only due to positional attention masking.
> 

All of this and much more might have happened - I’m just showing what I found, and reminding everyone that the specific experiments I ran say nothing about causation. Also, no examination of separability was made.

While I couldn't fully replicate the sophisticated analyses from the original paper, this exploration revealed interesting insights about how different models represent circular concepts. The clear temporal patterns in the original paper appear to be somewhat model-specific, and attempts to find similar patterns in other naturally circular domains produced mixed results.

I also have a feeling that applying PCA might make the 2D representations look less circular in some cases - sometimes it kind of looked like the point clouds were kind of rotated in 3D space, and as if they were actually showing up as a circle with “depth”. See for example the weekday experiment results for layer 21 in Gemma-2 9B. This doesn’t disprove any of the conclusions in the paper, but rather shows that other multidimensional geometrical representations might exist (for example cylindrical or helical structures).

For future work, it would be interesting to:

- Run the other experiments of Engels et al. on the Gemma models as well to see if the same causal mechanisms show up.
- Plot the PCA projections in three dimensions instead of two.
